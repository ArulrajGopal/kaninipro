{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e28d7364-cb6e-4dd3-8044-6f785da41e46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#setting up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "156d3325-66d4-4198-8e10-1821fbae4849",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "account_key = \"your_access_key\"\n",
    "spark.conf.set(\"fs.azure.account.key.kaninipro.dfs.core.windows.net\",account_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3165b58-d30b-419a-81e4-cd07bc0a97e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Read file with metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6cab207-6cb3-4dd3-b4b9-52621469704d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "city_weather = spark.read.format(\"parquet\")\\\n",
    "                .load(\"abfss://data@kaninipro.dfs.core.windows.net/city_weather_parquet\")\\\n",
    "                .selectExpr(\"*\",\n",
    "                            \"_metadata.file_path as file_path\",\n",
    "                            \"_metadata.file_name as file_name\",\n",
    "                            \"_metadata.file_modification_time as file_modification_time\",\n",
    "                            \"_metadata.file_size as file_size\",\n",
    "                            \"_metadata.file_block_length as file_block_length\",\n",
    "                            \"_metadata.file_block_start as file_block_start\"\n",
    "                            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a4f3b3e-8b3a-4d0d-81fd-b7dca7011d71",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\",\"city\"],\"right\":[]},\"columnSizing\":{\"file_name1\":291,\"file_path\":276},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763231197977}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(city_weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c96829fc-1f94-44f6-9db5-e2d4f541103f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#PySpark DataFrame equality functions for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b580b73-6f35-4be7-b0b6-dfef1546a567",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_expected = spark.createDataFrame(data=[(\"Alfred\", 1500), (\"Alfred\", 2500), (\"Anna\", \n",
    "500), (\"Anna\", 3000)], schema=[\"name\", \"amount\"])\n",
    "\n",
    "df_actual = spark.createDataFrame(data=[(\"Alfred\", 1200), (\"Alfred\", 2500), (\"Anna\", 500), \n",
    "(\"Anna\", 3000)], schema=[\"name\", \"amount\"])\n",
    "\n",
    "from pyspark.testing import assertDataFrameEqual\n",
    "\n",
    "assertDataFrameEqual(df_actual, df_expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "676d8636-9900-461a-aa2a-e6a78c38647b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_expected = spark.createDataFrame(data=[\n",
    "                                            (\"Alfred\", 1500), \n",
    "                                            (\"Alfred\", 2500), \n",
    "                                            (\"Anna\", 500), \n",
    "                                            (\"Anna\", 3000)], \n",
    "                                    schema=[\"name\", \"amount\"]\n",
    "                                    )\n",
    "\n",
    "df_actual = spark.createDataFrame(data=[\n",
    "                                 (\"Alfred\", 1200), \n",
    "                                 (\"Alfred\", 300), \n",
    "                                 (\"Anna\", 500), \n",
    "                                 (\"Anna\", 3000)], \n",
    "                            schema=[\"name\", \"amount\"]\n",
    "                            )\n",
    "\n",
    "\n",
    "from pyspark.testing import assertDataFrameEqual\n",
    "from pyspark.errors import PySparkAssertionError\n",
    "\n",
    "try:\n",
    "    assertDataFrameEqual(df_actual, df_expected, includeDiffRows=True)\n",
    "except PySparkAssertionError as e:\n",
    "    # `e.data` here looks like:\n",
    "    # [(Row(name='Alfred', amount=1200), Row(name='Alfred', amount=1500))]\n",
    "    errored_recrods = spark.createDataFrame(e.data, schema=[\"Actual\", \"Expected\"])\n",
    "\n",
    "display(errored_recrods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fb937cd-6a57-457b-a004-dec7f424ab44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema_actual = \"name STRING, amount DOUBLE\"\n",
    "\n",
    "data_expected = [[\"Alfred\", 1500], [\"Alfred\", 2500], [\"Anna\", 500], [\"Anna\", 3000]]\n",
    "data_actual = [[\"Alfred\", 1500.0], [\"Alfred\", 2500.0], [\"Anna\", 500.0], [\"Anna\", 3000.0]]\n",
    "\n",
    "df_expected = spark.createDataFrame(data = data_expected)\n",
    "df_actual = spark.createDataFrame(data = data_actual, schema = schema_actual)\n",
    "\n",
    "from pyspark.testing import assertSchemaEqual\n",
    "\n",
    "assertSchemaEqual(df_actual.schema, df_expected.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ee6bab6-a0c9-479f-9a82-9273fcef04b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#sort array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d52854b-f2a2-4bca-88cd-6686b95b60af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as sf\n",
    "df = spark.createDataFrame([([2, 1, None, 3],)], ['data'])\n",
    "df.select(sf.sort_array(df.data)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b709152-1c5a-43dd-ac3d-897ebe592641",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as sf\n",
    "df = spark.createDataFrame([([2, 1, None, 3],)], ['data'])\n",
    "df.select(sf.sort_array(df.data, asc=False)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9a5dbf1-ec83-4525-83e2-90d7ad97acfc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as sf\n",
    "df = spark.createDataFrame([([1],)], ['data'])\n",
    "df.select(sf.sort_array(df.data)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cabf8ffc-7274-420e-86da-a9b5eddb1013",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import functions as sf\n",
    "from pyspark.sql.types import ArrayType, StringType, StructField, StructType\n",
    "schema = StructType([StructField(\"data\", ArrayType(StringType()), True)])\n",
    "df = spark.createDataFrame([([],)], schema=schema)\n",
    "df.select(sf.sort_array(df.data)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "316ce6c8-64c7-40a7-901a-eb73662d8700",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as sf\n",
    "from pyspark.sql.types import ArrayType, IntegerType, StructType, StructField\n",
    "schema = StructType([StructField(\"data\", ArrayType(IntegerType()), True)])\n",
    "df = spark.createDataFrame([([None, None, None],)], schema=schema)\n",
    "df.select(sf.sort_array(df.data)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12f7a3ab-b021-4df9-b2c4-b45e36c0d57a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n",
    "from pyspark.sql.functions import sort_array\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"nested_array\", ArrayType(\n",
    "        StructType([\n",
    "            StructField(\"level1_field\", StringType(), True),\n",
    "            StructField(\"level2_struct\", StructType([\n",
    "                StructField(\"level2_field1\", IntegerType(), True),\n",
    "                StructField(\"level2_field2\", StringType(), True)\n",
    "            ]), True)\n",
    "        ])\n",
    "    ), True)\n",
    "])\n",
    "\n",
    "data = [\n",
    "    (1, [\n",
    "        {\"level1_field\": \"a\", \"level2_struct\": {\"level2_field2\": \"x\", \"level2_field1\": 10}},\n",
    "        {\"level1_field\": \"b\", \"level2_struct\": {\"level2_field1\": 20, \"level2_field2\": \"y\"}},\n",
    "        {\"level1_field\": \"z\", \"level2_struct\": {\"level2_field1\": 30, \"level2_field2\": \"z\"}},\n",
    "        {\"level1_field\": \"o\", \"level2_struct\": {\"level2_field1\": 40, \"level2_field2\": \"w\"}},\n",
    "        {\"level1_field\": \"e\", \"level2_struct\": {\"level2_field1\": 50, \"level2_field2\": \"v\"}}\n",
    "    ]),\n",
    "    (2, [\n",
    "        {\"level1_field\": \"f\", \"level2_struct\": {\"level2_field1\": 60, \"level2_field2\": \"u\"}},\n",
    "        {\"level1_field\": \"g\", \"level2_struct\": {\"level2_field1\": 70, \"level2_field2\": \"t\"}}\n",
    "    ]),\n",
    "    (3, [\n",
    "        {\"level1_field\": \"h\", \"level2_struct\": {\"level2_field1\": 80, \"level2_field2\": \"s\"}},\n",
    "        {\"level1_field\": \"i\", \"level2_struct\": {\"level2_field1\": 90, \"level2_field2\": \"r\"}},\n",
    "        {\"level1_field\": \"j\", \"level2_struct\": {\"level2_field1\": 100, \"level2_field2\": \"q\"}}\n",
    "    ])\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31f7cec4-68ed-4dc3-92e0-157e8b6cebd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c331f8c-6232-4464-a4cc-fd276c5ad7a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.select(\"id\",sort_array(sf.col(\"nested_array\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "560e7066-846f-4989-b4f9-bc15f0fd0942",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
