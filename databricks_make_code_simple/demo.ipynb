{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e28d7364-cb6e-4dd3-8044-6f785da41e46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#setting up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "156d3325-66d4-4198-8e10-1821fbae4849",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "account_key = \"your_access_key\"\n",
    "spark.conf.set(\"fs.azure.account.key.kaninipro.dfs.core.windows.net\",account_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3165b58-d30b-419a-81e4-cd07bc0a97e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Read file with metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6cab207-6cb3-4dd3-b4b9-52621469704d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "city_weather = spark.read.format(\"parquet\")\\\n",
    "                .load(\"abfss://data@kaninipro.dfs.core.windows.net/city_weather_parquet\")\\\n",
    "                .selectExpr(\"*\",\n",
    "                            \"_metadata.file_path as file_path\",\n",
    "                            \"_metadata.file_name as file_name\",\n",
    "                            \"_metadata.file_modification_time as file_modification_time\",\n",
    "                            \"_metadata.file_size as file_size\",\n",
    "                            \"_metadata.file_block_length as file_block_length\",\n",
    "                            \"_metadata.file_block_start as file_block_start\"\n",
    "                            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a4f3b3e-8b3a-4d0d-81fd-b7dca7011d71",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\",\"city\"],\"right\":[]},\"columnSizing\":{\"file_name1\":291,\"file_path\":276},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763231197977}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(city_weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce5b5aaf-b22c-44fa-8432-df2ce1518d3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#eqNullSafe for safe equality checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ceb7674-f6cc-4126-bfc0-89a6970fad2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, col\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (1, 1),\n",
    "    (None, None),\n",
    "    (None, 5),\n",
    "    (10, None)\n",
    "], [\"col1\", \"col2\"])\n",
    "\n",
    "col_added_df = df.withColumn(\"is_equal\", col(\"col1\").eqNullSafe(col(\"col2\")))\n",
    "\n",
    "display(col_added_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0cfef4d-3c2c-4bd2-8f99-72d3db13b0f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1 = spark.createDataFrame([\n",
    "    (1, \"A\"),\n",
    "    (None, \"B\"),\n",
    "    (3, \"C\")\n",
    "], [\"id\", \"value1\"])\n",
    "\n",
    "df2 = spark.createDataFrame([\n",
    "    (1, \"X\"),\n",
    "    (None, \"Y\"),\n",
    "    (4, \"Z\")\n",
    "], [\"id\", \"value2\"])\n",
    "\n",
    "result = df1.join(\n",
    "    df2,\n",
    "    df1[\"id\"].eqNullSafe(df2[\"id\"]),\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "display(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "827c4bac-0c14-47d1-9f61-cce42cf76b9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#sort array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b7a40f3-19b8-4706-9602-377ff96086b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n",
    "from pyspark.sql.functions import sort_array, col\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\n",
    "        \"array_field\",\n",
    "        ArrayType(\n",
    "            StructType([\n",
    "                StructField(\"field1\", IntegerType(), True),\n",
    "                StructField(\"field2\", StringType(), True)\n",
    "            ])\n",
    "        ),\n",
    "        True\n",
    "    )\n",
    "])\n",
    "\n",
    "data = [\n",
    "    (1, [\n",
    "        {\"field1\": 10, \"field2\": \"def\"},\n",
    "        {\"field1\": 70, \"field2\": \"ghi\"},\n",
    "        {\"field1\": 70, \"field2\": \"abc\"}\n",
    "    ]),\n",
    "    (2, [\n",
    "        {\"field1\": 10, \"field2\": \"ijk\"},\n",
    "        {\"field1\": 20, \"field2\": \"sdg\"},\n",
    "    ]),\n",
    "    (3, [\n",
    "        {\"field1\": 100, \"field2\": \"abc\"},\n",
    "        {\"field1\": 5, \"field2\": \"ood\"},\n",
    "        {\"field1\": 5, \"field2\": \"afe\"}\n",
    "    ])\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d25502b1-e26b-4fd1-a0e4-42699cca3e88",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"array_field\":485},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763268319094}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9466a8ef-174f-41c5-9956-685ea8eff6fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.select(\"id\",sort_array(col(\"array_field\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b560761-11dd-45b9-9811-298570acc3f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Apply inline transform on arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6786a079-b7f6-4f68-a557-8dbc1b27aaeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59babf69-afb6-4d3b-a83a-9c4a4e8d067a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n",
    "from pyspark.sql.functions import col,expr\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\n",
    "        \"left_array\",\n",
    "        ArrayType(\n",
    "            StructType([\n",
    "                StructField(\"id\", IntegerType(), True),\n",
    "                StructField(\"value\", StringType(), True)\n",
    "            ])\n",
    "        ),\n",
    "        True\n",
    "    ),\n",
    "     StructField(\n",
    "        \"right_array\",\n",
    "        ArrayType(\n",
    "            StructType([\n",
    "                StructField(\"id\", IntegerType(), True),\n",
    "                StructField(\"value\", StringType(), True)\n",
    "            ])\n",
    "        ),\n",
    "        True\n",
    "    )\n",
    "])\n",
    "\n",
    "data = [\n",
    "    (1, [\n",
    "        {\"id\": 1, \"value\": \"def\"},\n",
    "        {\"id\": 2, \"value\": \"ghi\"},\n",
    "        {\"id\": 3, \"value\": \"abc\"}\n",
    "    ],\n",
    "     [\n",
    "        {\"id\": 1, \"value\": \"def\"},\n",
    "        {\"id\": 2, \"value\": \"abc\"}\n",
    "    ]\n",
    "     ),\n",
    "    (2, [\n",
    "        {\"id\": 1, \"value\": \"ijk\"},\n",
    "        {\"id\": 2, \"value\": \"sdg\"},\n",
    "    ],\n",
    "     [\n",
    "        {\"id\": 1, \"value\": \"ijk\"}\n",
    "    ],\n",
    "     )\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "984a041d-e129-4f21-a785-4195dc3c84be",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"inverse_intersection\":266,\"left_array\":201},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763278619129}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "missing_values_df = (\n",
    "    df.withColumn(\n",
    "        \"inverse_intersection\",\n",
    "        expr(\"filter(left_array, x -> NOT array_contains(right_array, x))\")\n",
    "    )\n",
    "    .withColumn(\"is_changed\", expr(\"size(inverse_intersection) <> 0\"))\n",
    ")\n",
    "\n",
    "display(missing_values_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a77df3f1-d21b-4e36-83eb-92404cf11d17",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"inverse_intersection\":233,\"left_array\":201,\"id\":90},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763278509343}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "missing_values_df = df.withColumn(\n",
    "    \"inverse_intersection\",\n",
    "    expr(\"\"\"\n",
    "        filter(\n",
    "            left_array,\n",
    "            x -> x.value <> filter(right_array, y -> y.id = x.id)[0].value\n",
    "        )\n",
    "    \"\"\")\n",
    ")\\\n",
    ".withColumn(\"is_changed\",expr(\"size(inverse_intersection) <> 0\"))\n",
    "display(missing_values_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "681ec227-a423-4351-9dd4-fac882e972da",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"left_array\":235,\"new_array\":274},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763278989270}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "transformed_df = df.withColumn(\n",
    "    \"new_array\",\n",
    "    expr(\"\"\"\n",
    "        transform(\n",
    "            left_array,\n",
    "            x -> \n",
    "            named_struct (\n",
    "                \"id\", x.id + 5,\n",
    "                \"value\", concat(x.value, '_new')\n",
    "                )\n",
    "        )\n",
    "    \"\"\")\n",
    ")\n",
    "\n",
    "display(transformed_df.select(\"left_array\", \"new_array\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73a0c7f5-6cd9-4d26-a867-9d2d635ba51a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##scala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "986f63ae-16ef-4c7c-8e28-5519c4211e0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val schema = StructType(Array(\n",
    "  StructField(\"id\", IntegerType, true),\n",
    "  StructField(\n",
    "    \"left_array\",\n",
    "    ArrayType(\n",
    "      StructType(Array(\n",
    "        StructField(\"id\", IntegerType, true),\n",
    "        StructField(\"value\", StringType, true)\n",
    "      ))\n",
    "    ),\n",
    "    true\n",
    "  ),\n",
    "  StructField(\n",
    "    \"right_array\",\n",
    "    ArrayType(\n",
    "      StructType(Array(\n",
    "        StructField(\"id\", IntegerType, true),\n",
    "        StructField(\"value\", StringType, true)\n",
    "      ))\n",
    "    ),\n",
    "    true\n",
    "  )\n",
    "))\n",
    "\n",
    "val data = Seq(\n",
    "  Row(1, Seq(Row(1,\"def\"), Row(2, \"ghi\"), Row(3, \"abc\")), Seq(Row(1,\"def\"), Row(2, \"abc\"))),\n",
    "  Row(2, Seq(Row(1,\"ijk\"), Row(2, \"sdg\")), Seq(Row(1,\"ijk\"))),\n",
    ")\n",
    "\n",
    "\n",
    "val df = spark.createDataFrame(spark.sparkContext.parallelize(data), schema)\n",
    "\n",
    "display(df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f18652a2-a647-4ce7-b1a8-129a5d75220f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c96829fc-1f94-44f6-9db5-e2d4f541103f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#PySpark DataFrame equality functions for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fb937cd-6a57-457b-a004-dec7f424ab44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema_actual = \"name STRING, amount DOUBLE\"\n",
    "\n",
    "data_expected = [[\"Alfred\", 1500], [\"Alfred\", 2500], [\"Anna\", 500], [\"Anna\", 3000]]\n",
    "data_actual = [[\"Alfred\", 1500.0], [\"Alfred\", 2500.0], [\"Anna\", 500.0], [\"Anna\", 3000.0]]\n",
    "\n",
    "df_expected = spark.createDataFrame(data = data_expected)\n",
    "df_actual = spark.createDataFrame(data = data_actual, schema = schema_actual)\n",
    "\n",
    "from pyspark.testing import assertSchemaEqual\n",
    "\n",
    "assertSchemaEqual(df_actual.schema, df_expected.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b580b73-6f35-4be7-b0b6-dfef1546a567",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_expected = spark.createDataFrame(data=[(\"Alfred\", 1500), (\"Alfred\", 2500), (\"Anna\", \n",
    "500), (\"Anna\", 3000)], schema=[\"name\", \"amount\"])\n",
    "\n",
    "df_actual = spark.createDataFrame(data=[(\"Alfred\", 1200), (\"Alfred\", 2500), (\"Anna\", 500), \n",
    "(\"Anna\", 3000)], schema=[\"name\", \"amount\"])\n",
    "\n",
    "from pyspark.testing import assertDataFrameEqual\n",
    "\n",
    "assertDataFrameEqual(df_actual, df_expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "676d8636-9900-461a-aa2a-e6a78c38647b",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"Actual\":232,\"Expected\":246},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763267506934}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_expected = spark.createDataFrame(data=[\n",
    "                                            (\"Alfred\", 1500), \n",
    "                                            (\"Alfred\", 2500), \n",
    "                                            (\"Anna\", 500), \n",
    "                                            (\"Anna\", 3000)], \n",
    "                                    schema=[\"name\", \"amount\"]\n",
    "                                    )\n",
    "\n",
    "df_actual = spark.createDataFrame(data=[\n",
    "                                 (\"Alfred\", 1200), \n",
    "                                 (\"Alfred\", 300), \n",
    "                                 (\"Anna\", 500), \n",
    "                                 (\"Anna\", 3000)], \n",
    "                            schema=[\"name\", \"amount\"]\n",
    "                            )\n",
    "\n",
    "\n",
    "from pyspark.testing import assertDataFrameEqual\n",
    "from pyspark.errors import PySparkAssertionError\n",
    "\n",
    "try:\n",
    "    assertDataFrameEqual(df_actual, df_expected, includeDiffRows=True)\n",
    "except PySparkAssertionError as e:\n",
    "    # `e.data` here looks like:\n",
    "    # [(Row(name='Alfred', amount=1200), Row(name='Alfred', amount=1500))]\n",
    "    errored_recrods = spark.createDataFrame(e.data, schema=[\"Actual\", \"Expected\"])\n",
    "\n",
    "display(errored_recrods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3262ec4a-daeb-44f1-9b49-f412d52f3bbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#prefer unionByName over union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "560e7066-846f-4989-b4f9-bc15f0fd0942",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1 = spark.createDataFrame(\n",
    "    [(1, \"Arul\", 30)],\n",
    "    [\"id\", \"name\", \"age\"]\n",
    ")\n",
    "\n",
    "df2 = spark.createDataFrame(\n",
    "    [(2, 28, \"Meena\")],   # Notice: order is different → (id, age, name)\n",
    "    [\"id\", \"age\", \"name\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "763b7f0b-468d-464b-96ea-9212de246397",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df1.union(df2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6247ed4c-8703-452e-a007-e903b0689f2b",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763269923105}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df1.unionByName(df2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30fe2479-67b7-4f78-a3f0-dbe1d7affd02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##handling missing columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a022abd2-4e59-4ef4-b71b-28c841adb0de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1 = spark.createDataFrame(\n",
    "    [(1, 30)],\n",
    "    [\"id\", \"age\"]\n",
    ")\n",
    "\n",
    "df2 = spark.createDataFrame(\n",
    "    [(2, 28, \"Meena\")],   # Notice: order is different → (id, age, name)\n",
    "    [\"id\", \"age\", \"name\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3d68d85-950b-4372-9c1a-6befdc7f94b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df1.unionByName(df2, allowMissingColumns=True))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "demo",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
